{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:48:13.726743Z",
     "start_time": "2017-09-30T19:48:12.512388Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pdir as pr\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "def pl(myList):\n",
    "    '''tiny function to print my list in a truncated way'''\n",
    "    print(myList, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-29T23:10:39.217172Z",
     "start_time": "2017-09-29T23:10:39.212668Z"
    }
   },
   "source": [
    "# 分词\n",
    "分词，它产生我们所熟悉的结构，一个词汇和标点符号的链表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:33:36.341409Z",
     "start_time": "2017-09-30T15:33:36.151616Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Viterbi', 'Algorithm', 'is', 'a', 'dynamic', 'programming', 'algorithm', 'for', 'finding', 'the', 'most', 'likely', 'sequence', 'of', 'hidden', 'states', '–', 'called', 'the', 'Viterbi', 'path', '–', 'that', 'results', 'in', 'a', 'sequence', 'of', 'observed', 'events', ',', 'especially', 'in', 'the', 'context', 'of', 'Markov', 'information', 'sources', 'and', 'hidden', 'Markov', 'models', '.'] "
     ]
    }
   ],
   "source": [
    "testStr = '''\n",
    "The Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states \n",
    "– called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov \n",
    "information sources and hidden Markov models.\n",
    "'''\n",
    "tokens = nltk.word_tokenize(testStr) #一个词汇和标点符号的链表\n",
    "pl(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-29T23:19:32.506627Z",
     "start_time": "2017-09-29T23:19:32.499102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: The Viterbi Algorithm is a dynamic programming algorithm...>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.Text(tokens) #转换为Text，以实现前两章的文本分析方法\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-29T23:19:49.249257Z",
     "start_time": "2017-09-29T23:19:49.238250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;33mabstract class:\u001b[0m\n",
       "    \u001b[0;36m__subclasshook__\u001b[0m\n",
       "\u001b[0;33mattribute access:\u001b[0m\n",
       "    \u001b[0;36m__delattr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__dir__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__getattribute__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__setattr__\u001b[0m\n",
       "\u001b[0;33memulating container:\u001b[0m\n",
       "    \u001b[0;36m__getitem__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__len__\u001b[0m\n",
       "\u001b[0;33mobject customization:\u001b[0m\n",
       "    \u001b[0;36m__format__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__hash__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__init__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__new__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__repr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__sizeof__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__str__\u001b[0m\n",
       "\u001b[0;33mother:\u001b[0m\n",
       "    \u001b[0;36m_CONTEXT_RE\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_COPY_TOKENS\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mname\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mtokens\u001b[0m\n",
       "\u001b[0;33mpickle:\u001b[0m\n",
       "    \u001b[0;36m__reduce__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__reduce_ex__\u001b[0m\n",
       "\u001b[0;33mrich comparison:\u001b[0m\n",
       "    \u001b[0;36m__eq__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ge__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__gt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__le__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__lt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ne__\u001b[0m\n",
       "\u001b[0;33mspecial attribute:\u001b[0m\n",
       "    \u001b[0;36m__class__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__dict__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__doc__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__module__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__weakref__\u001b[0m\n",
       "\u001b[0;33mfunction:\u001b[0m\n",
       "    \u001b[0;36m__unicode__:\u001b[0m \u001b[1;30mReturn str(self).\u001b[0m\n",
       "    \u001b[0;36m_context:\u001b[0m \u001b[1;30mOne left & one right token, both case-normalized.  Skip over\u001b[0m\n",
       "    \u001b[0;36mcollocations:\u001b[0m \u001b[1;30mPrint collocations derived from the text, ignoring stopwords.\u001b[0m\n",
       "    \u001b[0;36mcommon_contexts:\u001b[0m \u001b[1;30mFind contexts where the specified words appear; list\u001b[0m\n",
       "    \u001b[0;36mconcordance:\u001b[0m \u001b[1;30mPrint a concordance for ``word`` with the specified context window.\u001b[0m\n",
       "    \u001b[0;36mcount:\u001b[0m \u001b[1;30mCount the number of times this word appears in the text.\u001b[0m\n",
       "    \u001b[0;36mdispersion_plot:\u001b[0m \u001b[1;30mProduce a plot showing the distribution of the words through the text.\u001b[0m\n",
       "    \u001b[0;36mfindall:\u001b[0m \u001b[1;30mFind instances of the regular expression in the text.\u001b[0m\n",
       "    \u001b[0;36mindex:\u001b[0m \u001b[1;30mFind the index of the first occurrence of the word in the text.\u001b[0m\n",
       "    \u001b[0;36mplot:\u001b[0m \u001b[1;30mSee documentation for FreqDist.plot()\u001b[0m\n",
       "    \u001b[0;36mreadability:\u001b[0m \u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36msimilar:\u001b[0m \u001b[1;30mDistributional similarity: find other words which appear in the\u001b[0m\n",
       "    \u001b[0;36municode_repr:\u001b[0m \u001b[1;30mReturn repr(self).\u001b[0m\n",
       "    \u001b[0;36mvocab:\u001b[0m \u001b[1;30m:seealso: nltk.prob.FreqDist\u001b[0m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:29:56.877320Z",
     "start_time": "2017-09-30T15:29:56.867312Z"
    },
    "collapsed": true
   },
   "source": [
    "# 常见的字符串操作函数\n",
    "\n",
    "```c++\n",
    "方法          功能\n",
    "s.find(t)     字符串 s 中包含 t 的第一个索引（没找到返回-1）\n",
    "s.rfind(t)    字符串 s 中包含 t 的最后一个索引（没找到返回-1）\n",
    "s.index(t)    与 s.find(t)功能类似，但没找到时引起 ValueError\n",
    "s.rindex(t)   与 s.rfind(t)功能类似，但没找到时引起 ValueError\n",
    "s.join(text)  连接字符串 s 与 text 中的词汇\n",
    "s.split(t)    在所有找到 t 的位置将 s 分割成链表（默认为空白符）\n",
    "s.splitlines() 将 s 按行分割成字符串链表\n",
    "s.lower()     将字符串 s 小写\n",
    "s.upper()     将字符串 s 大写\n",
    "s.titlecase() 将字符串 s 首字母大写\n",
    "s.strip()     返回一个没有首尾空白字符的 s 的拷贝\n",
    "s.replace(t, u) 用 u 替换 s 中的 t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:37:46.769620Z",
     "start_time": "2017-09-30T15:37:46.756585Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e2aa76828d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestStr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "testStr[0]='a' #字符串是不可变的：一旦你创建了一个字符串，就不能改变它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:34:14.728684Z",
     "start_time": "2017-09-30T15:34:14.717678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states \\n– called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov \\ninformation sources and hidden Markov models.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['The Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states ',\n",
       " '– called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov ',\n",
       " 'information sources and hidden Markov models.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testStr\n",
    "testStr.strip().splitlines() #按行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:35:20.648516Z",
     "start_time": "2017-09-30T15:35:20.589459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;33mabstract class:\u001b[0m\n",
       "    \u001b[0;36m__subclasshook__\u001b[0m\n",
       "\u001b[0;33marithmetic:\u001b[0m\n",
       "    \u001b[0;36m__add__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__mod__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__mul__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__rmod__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__rmul__\u001b[0m\n",
       "\u001b[0;33mattribute access:\u001b[0m\n",
       "    \u001b[0;36m__delattr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__dir__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__getattribute__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__setattr__\u001b[0m\n",
       "\u001b[0;33memulating container:\u001b[0m\n",
       "    \u001b[0;36m__contains__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__getitem__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__iter__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__len__\u001b[0m\n",
       "\u001b[0;33mobject customization:\u001b[0m\n",
       "    \u001b[0;36m__format__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__hash__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__init__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__new__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__repr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__sizeof__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__str__\u001b[0m\n",
       "\u001b[0;33mpickle:\u001b[0m\n",
       "    \u001b[0;36m__getnewargs__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__reduce__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__reduce_ex__\u001b[0m\n",
       "\u001b[0;33mrich comparison:\u001b[0m\n",
       "    \u001b[0;36m__eq__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ge__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__gt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__le__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__lt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ne__\u001b[0m\n",
       "\u001b[0;33mspecial attribute:\u001b[0m\n",
       "    \u001b[0;36m__class__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__doc__\u001b[0m\n",
       "\u001b[0;33mfunction:\u001b[0m\n",
       "    \u001b[0;36mcapitalize:\u001b[0m \u001b[1;30mS.capitalize() -> str\u001b[0m\n",
       "    \u001b[0;36mcasefold:\u001b[0m \u001b[1;30mS.casefold() -> str\u001b[0m\n",
       "    \u001b[0;36mcenter:\u001b[0m \u001b[1;30mS.center(width[, fillchar]) -> str\u001b[0m\n",
       "    \u001b[0;36mcount:\u001b[0m \u001b[1;30mS.count(sub[, start[, end]]) -> int\u001b[0m\n",
       "    \u001b[0;36mencode:\u001b[0m \u001b[1;30mS.encode(encoding='utf-8', errors='strict') -> bytes\u001b[0m\n",
       "    \u001b[0;36mendswith:\u001b[0m \u001b[1;30mS.endswith(suffix[, start[, end]]) -> bool\u001b[0m\n",
       "    \u001b[0;36mexpandtabs:\u001b[0m \u001b[1;30mS.expandtabs(tabsize=8) -> str\u001b[0m\n",
       "    \u001b[0;36mfind:\u001b[0m \u001b[1;30mS.find(sub[, start[, end]]) -> int\u001b[0m\n",
       "    \u001b[0;36mformat:\u001b[0m \u001b[1;30mS.format(*args, **kwargs) -> str\u001b[0m\n",
       "    \u001b[0;36mformat_map:\u001b[0m \u001b[1;30mS.format_map(mapping) -> str\u001b[0m\n",
       "    \u001b[0;36mindex:\u001b[0m \u001b[1;30mS.index(sub[, start[, end]]) -> int\u001b[0m\n",
       "    \u001b[0;36misalnum:\u001b[0m \u001b[1;30mS.isalnum() -> bool\u001b[0m\n",
       "    \u001b[0;36misalpha:\u001b[0m \u001b[1;30mS.isalpha() -> bool\u001b[0m\n",
       "    \u001b[0;36misdecimal:\u001b[0m \u001b[1;30mS.isdecimal() -> bool\u001b[0m\n",
       "    \u001b[0;36misdigit:\u001b[0m \u001b[1;30mS.isdigit() -> bool\u001b[0m\n",
       "    \u001b[0;36misidentifier:\u001b[0m \u001b[1;30mS.isidentifier() -> bool\u001b[0m\n",
       "    \u001b[0;36mislower:\u001b[0m \u001b[1;30mS.islower() -> bool\u001b[0m\n",
       "    \u001b[0;36misnumeric:\u001b[0m \u001b[1;30mS.isnumeric() -> bool\u001b[0m\n",
       "    \u001b[0;36misprintable:\u001b[0m \u001b[1;30mS.isprintable() -> bool\u001b[0m\n",
       "    \u001b[0;36misspace:\u001b[0m \u001b[1;30mS.isspace() -> bool\u001b[0m\n",
       "    \u001b[0;36mistitle:\u001b[0m \u001b[1;30mS.istitle() -> bool\u001b[0m\n",
       "    \u001b[0;36misupper:\u001b[0m \u001b[1;30mS.isupper() -> bool\u001b[0m\n",
       "    \u001b[0;36mjoin:\u001b[0m \u001b[1;30mS.join(iterable) -> str\u001b[0m\n",
       "    \u001b[0;36mljust:\u001b[0m \u001b[1;30mS.ljust(width[, fillchar]) -> str\u001b[0m\n",
       "    \u001b[0;36mlower:\u001b[0m \u001b[1;30mS.lower() -> str\u001b[0m\n",
       "    \u001b[0;36mlstrip:\u001b[0m \u001b[1;30mS.lstrip([chars]) -> str\u001b[0m\n",
       "    \u001b[0;36mmaketrans:\u001b[0m \u001b[1;30mReturn a translation table usable for str.translate().\u001b[0m\n",
       "    \u001b[0;36mpartition:\u001b[0m \u001b[1;30mS.partition(sep) -> (head, sep, tail)\u001b[0m\n",
       "    \u001b[0;36mreplace:\u001b[0m \u001b[1;30mS.replace(old, new[, count]) -> str\u001b[0m\n",
       "    \u001b[0;36mrfind:\u001b[0m \u001b[1;30mS.rfind(sub[, start[, end]]) -> int\u001b[0m\n",
       "    \u001b[0;36mrindex:\u001b[0m \u001b[1;30mS.rindex(sub[, start[, end]]) -> int\u001b[0m\n",
       "    \u001b[0;36mrjust:\u001b[0m \u001b[1;30mS.rjust(width[, fillchar]) -> str\u001b[0m\n",
       "    \u001b[0;36mrpartition:\u001b[0m \u001b[1;30mS.rpartition(sep) -> (head, sep, tail)\u001b[0m\n",
       "    \u001b[0;36mrsplit:\u001b[0m \u001b[1;30mS.rsplit(sep=None, maxsplit=-1) -> list of strings\u001b[0m\n",
       "    \u001b[0;36mrstrip:\u001b[0m \u001b[1;30mS.rstrip([chars]) -> str\u001b[0m\n",
       "    \u001b[0;36msplit:\u001b[0m \u001b[1;30mS.split(sep=None, maxsplit=-1) -> list of strings\u001b[0m\n",
       "    \u001b[0;36msplitlines:\u001b[0m \u001b[1;30mS.splitlines([keepends]) -> list of strings\u001b[0m\n",
       "    \u001b[0;36mstartswith:\u001b[0m \u001b[1;30mS.startswith(prefix[, start[, end]]) -> bool\u001b[0m\n",
       "    \u001b[0;36mstrip:\u001b[0m \u001b[1;30mS.strip([chars]) -> str\u001b[0m\n",
       "    \u001b[0;36mswapcase:\u001b[0m \u001b[1;30mS.swapcase() -> str\u001b[0m\n",
       "    \u001b[0;36mtitle:\u001b[0m \u001b[1;30mS.title() -> str\u001b[0m\n",
       "    \u001b[0;36mtranslate:\u001b[0m \u001b[1;30mS.translate(table) -> str\u001b[0m\n",
       "    \u001b[0;36mupper:\u001b[0m \u001b[1;30mS.upper() -> str\u001b[0m\n",
       "    \u001b[0;36mzfill:\u001b[0m \u001b[1;30mS.zfill(width) -> str\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr(testStr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unicode\n",
    "\n",
    "文件中的文本都是有特定编码的，所以我们需要一些机制来将文本翻译成 Unicode——翻译成 Unicode 叫做解码。相对的，要将 Unicode 写入一个文件或终端，我们首先需要将 Unicode 转化为合适的编码——这种将 Unicode 转化为其它编码的过程叫做编码。\n",
    "\n",
    "\n",
    "![unicode encode and decode](./images/3.jpg)\n",
    "\n",
    "从 Unicode 的角度来看，字符是可以实现一个或多个字形的抽象的实体，即是保存到内存中的。\n",
    "\n",
    "只有字形可以出现在屏幕上或被打印在纸上，因此对应的字符需要编码后才能合适输出。\n",
    "\n",
    "一个字体（对应不同编码方式）是一个字符到字形映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:57:46.301449Z",
     "start_time": "2017-09-30T15:57:46.262899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using unicode_escape\n",
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n",
      "\n",
      "using default\n",
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
     ]
    }
   ],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "#选择用latin2对文件进行编码\n",
    "with open(path, 'r', encoding='latin2') as f:\n",
    "    print('using unicode_escape')\n",
    "    temp=[]\n",
    "    for line in f.readlines():\n",
    "        print(line.strip().encode('unicode_escape'))\n",
    "        temp.append(line)\n",
    "        \n",
    "    print('\\nusing default')\n",
    "    for line in temp:\n",
    "        print(line.strip())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要在终端上查看这个文本，我们需要使用合适的编码对它进行编码。\n",
    "\n",
    "Python 特定的编码unicode_escape是一个虚拟的编码，它把所有非 ASCII 字符转换成它们的\\uXXXX 形式。\n",
    "\n",
    "编码点在ASCII码0-127 的范围以外但低于 256 的使用两位数字的形式\\xXX 表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T16:00:10.614456Z",
     "start_time": "2017-09-30T16:00:10.606950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ord()查找一个字符的整数序数\n",
    "ord('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T16:05:23.554743Z",
     "start_time": "2017-09-30T16:05:23.541735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ń'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "b'\\xc5\\x84'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"b'\\\\xc5\\\\x84'\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ = '\\u0144'\n",
    "n_ #试图显示字形\n",
    "n_.encode('utf-8')\n",
    "repr(n_.encode('utf-8'))#repr()输出 UTF-8 转义序列（以\\xXX 的形式），而不是试图显示字形，不过貌似没啥用in py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 正则表达式\n",
    "\n",
    "我们可以使用一种特殊的正则表达式搜索一个文本中多个词（这里的文本是一个标识符列表）。\n",
    "\n",
    "例如：```<a> <man>```找出文本中所有“a man”的实例。\n",
    "\n",
    "尖括号用于标记标识符的边界，尖括号之间的所有空白都被忽略（**这只对 NLTK 中的 findall()方法处理文本有效**）。\n",
    "\n",
    "使用<.*>，它将匹配所有单个标识符，将它括在括号里，于是只匹配词（例如：monied）而不匹配短语（例如：a monied man）\n",
    "\n",
    "至于其他的正则使用方法，这里不再深究，自行查阅教程学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:01:41.217613Z",
     "start_time": "2017-09-30T17:01:39.546595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 规范化文本\n",
    "## 转换大小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:09:54.703403Z",
     "start_time": "2017-09-30T17:09:54.697382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'god save me'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'GOD save me'.lower() #转换为小写，非常重要的一招"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 词干提取器\n",
    "\n",
    "NLTK 中包括了一些现成的词干提取器，如果你需要一个词干提取器，你应该优先使用它们中的一个，而不是使用正则表达式制作自己的词干提取器，因为 NLTK 中的词干提取器能处理的不规则的情况很广泛。\n",
    "\n",
    "Porter 和 Lancaster 词干提取器按照它们自己的规则剥离词缀。Porter 词干提取器正确处理了词 lying（将它映射为 lie），而 Lancaster 词干提取器并没有处理好。\n",
    "\n",
    "词干提取过程没有明确定义，我们通常选择心目中最适合我们的应用的词干提取器。如果你要索引一些文本和使搜索支持不同词汇形式的话，Porter 词干提取器是一个很好的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:31:18.864037Z",
     "start_time": "2017-09-30T17:31:18.836008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS:', 'Listen,', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', '', '', '', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government.', 'Supreme', 'executive', 'power', 'derives', 'from', '', '', '', 'a', 'mandate', 'from', 'the', 'masses,', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony.'] "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-------------------------------'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.'] "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-------------------------------'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.'] "
     ]
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "    is no basis for a system of government. Supreme executive power derives from\n",
    "    a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "pl(raw.replace('\\n','').split(' '))\n",
    "'-------------------------------'\n",
    "pl([porter.stem(t) for t in tokens])\n",
    "'-------------------------------'\n",
    "pl([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:40:15.052426Z",
     "start_time": "2017-09-30T17:40:14.796021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "# 使用词干提取器索引文本\n",
    "class IndexedText(object):\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i) for (i, word) in enumerate(text))\n",
    "        \n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4) # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '%*s' % (width, lcontext[-width:])\n",
    "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
    "            print(ldisplay, rdisplay)\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## WordNet词形归并\n",
    "\n",
    "WordNet词形归并器删除词缀产生的词都是在它的字典中的词。这个额外的检查过程使词形归并器比刚才提到的词干提取器要慢。请注意，它并没有处理“lying”，但它将“women”转换为“woman”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:29:29.455797Z",
     "start_time": "2017-09-30T17:29:26.671277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.'] "
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "pl([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用正则表达式为文本分词\n",
    "\n",
    "正则表达式符号\n",
    "```\n",
    "符号 功能\n",
    "\\b 词边界（零宽度）\n",
    "\\d 任一十进制数字（相当于[0-9]）\n",
    "\\D 任何非数字字符（等价于[^ 0-9]）\n",
    "\\s 任何空白字符（相当于[ \\t\\n\\r\\f\\v]）\n",
    "\\S 任何非空白字符（相当于[^ \\t\\n\\r\\f\\v]）\n",
    "\\w 任何字母数字字符（相当于[a-zA-Z0-9_]）\n",
    "\\W 任何非字母数字字符（相当于[^a-zA-Z0-9_]）\n",
    "\\t 制表符\n",
    "\\n 换行符\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:51:32.027335Z",
     "start_time": "2017-09-30T17:51:32.004291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'When I'M a Duchess,' she said to\\therself, (not in a very hopeful tone\\n   though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\\n   well without--Maybe it's always pepper that makes people hot-tempered,'...\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"] "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'----------------------------'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', ''] "
     ]
    }
   ],
   "source": [
    "import re\n",
    "raw = \"\"\"'When I'M a Duchess,' she said to\\therself, (not in a very hopeful tone\n",
    "   though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "   well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "raw\n",
    "pl(re.split(r'\\s+', raw)) # \\s可匹配 回车、换行、空白等空白字符\n",
    "'----------------------------'\n",
    "pl(re.split(r'\\W+', raw)) # \\W可匹配 所有字母、数字和下划线以外的字符 注意是大写的W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 的正则表达式分词器\n",
    "\n",
    "函数 nltk.regexp_tokenize()与 re.findall()类似（我们一直在使用它进行分词）。然而，nltk.regexp_tokenize()分词效率更高，且不需要特殊处理括号。\n",
    "\n",
    " verbose 标志时，可以不再使用' '来匹配一个空格字符；使用“\\s”代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T18:51:58.959586Z",
     "start_time": "2017-09-30T18:51:58.943583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)       # set flag to allow verbose regexps #去掉嵌入的空白字符和注释\n",
    "    ((?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "    | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "    | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.             # ellipsis\n",
    "    | [][.,;\"'?():-_`])  # these are separate tokens\n",
    "    '''\n",
    "wordlist = nltk.regexp_tokenize(text, pattern) #这个正则是真的考验\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用 set(tokens).difference(wordlist) 来评估两种分词的区别，可见效果还是很接近的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T18:52:41.278036Z",
     "start_time": "2017-09-30T18:52:41.266019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'$', '12.40'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tokens\n",
    "set(tokens).difference(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面以《华尔街日报》为原始语料用正则表达式分词，并将结果和分好词的结果进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:05:50.002044Z",
     "start_time": "2017-09-30T19:05:47.977944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1229"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = nltk.corpus.treebank_raw.raw()\n",
    "wordlist2 = nltk.regexp_tokenize(text2, pattern)\n",
    "tokens2 = nltk.corpus.treebank.words()\n",
    "len(set(tokens2).difference(wordlist2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词的最后一个问题是缩写的存在，如“didn't”。如果我们想分析一个句子的意思，将\n",
    "这种形式规范化为两个独立的形式：“did”和“n't”(不是 not)可能更加有用。我们可以通过\n",
    "查表来做这项工作。\n",
    "\n",
    "# 断句\n",
    "\n",
    "断句是困难的，因为句号会被用来标记缩写而另一些句号同时标记缩写和句子结束，就像发生在缩写如“U.S.A.”上的那样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:14:36.766749Z",
     "start_time": "2017-09-30T19:14:36.552404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[The Man Who Was Thursday by G. K. Chesterton 1908]\\n\\nTo Edmund Clerihew Bentley\\n\\nA cloud was on the mind of men, and wailing went the weather,\\nYea, a sick cloud upon the soul when we were boys together.',\n",
       " 'Science announced nonentity and art admired decay;\\nThe world was old and ended: but you and I were gay;\\nRound us in antic order their crippled vices came--\\nLust that had lost its laughter, fear that had lost its shame.',\n",
       " 'Like the white lock of Whistler, that lit our aimless gloom,\\nMen showed their own white feather as proudly as a plume.',\n",
       " 'Life was a fly that faded, and death a drone that stung;\\nThe world was very old indeed when you and I were young.',\n",
       " 'They twisted even decent sin to shapes not to be named:\\nMen were ashamed of honour; but we were not ashamed.',\n",
       " 'Weak if we were and foolish, not thus we failed, not thus;\\nWhen that black Baal blocked the heavens he had no hymns from us\\nChildren we were--our forts of sand were even as weak as eve,\\nHigh as they went we piled them up to break that bitter sea.',\n",
       " 'Fools as we were in motley, all jangling and absurd,\\nWhen all church bells were silent our cap and beds were heard.',\n",
       " 'Not all unhelped we held the fort, our tiny flags unfurled;\\nSome giants laboured in that cloud to lift it from the world.',\n",
       " 'I find again the book we found, I feel the hour that flings\\nFar out of fish-shaped Paumanok some cry of cleaner things;\\nAnd the Green Carnation withered, as in forest fires that pass,\\nRoared in the wind of all the world ten million leaves of grass;\\nOr sane and sweet and sudden as a bird sings in the rain--\\nTruth out of Tusitala spoke and pleasure out of pain.',\n",
       " 'Yea, cool and clear and sudden as a bird sings in the grey,\\nDunedin to Samoa spoke, and darkness unto day.']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle') #加载分词器\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "sents[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词 - 模拟退火"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:48:21.155188Z",
     "start_time": "2017-09-30T19:48:21.125161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy'] "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-------------------'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you', 'like', 'the', 'kitty', 'like', 'the', 'doggy'] "
     ]
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\" #原始无边界待分割的字符串\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\" #以1代表分隔边界\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\" \n",
    "\n",
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words\n",
    "\n",
    "pl(segment(text,seg1))\n",
    "'-------------------'\n",
    "pl(segment(text,seg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一件有趣的事情发生了，现在分词问题就转变为找到一个合适的01序列使其对应的分词结果最优，也即是一个搜索问题。\n",
    "\n",
    "因此可使用模拟退火算法求解较优解。\n",
    "\n",
    "首先要定义一个根据结果进行打分的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:51:16.252832Z",
     "start_time": "2017-09-30T19:51:16.208804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后的词:\n",
      " ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "\n",
      "分词后词的个数:  4\n",
      "分词后加上空格的句子总长度:  59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后的词:\n",
      " ['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "\n",
      "分词后词的个数:  16\n",
      "分词后加上空格的句子总长度:  31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(text, segs, show_details=False):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = len(' '.join(list(set(words))))\n",
    "    if show_details:\n",
    "        print('分词后的词:\\n', words)\n",
    "        print('\\n分词后词的个数: ',text_size)\n",
    "        print('分词后加上空格的句子总长度: ',lexicon_size)\n",
    "    return text_size + lexicon_size\n",
    "\n",
    "evaluate(text, seg1, show_details=True)\n",
    "evaluate(text, seg2, show_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用模拟退火算法的非确定性搜索：\n",
    "一开始仅搜索短语分词；随机扰动 0 和 1，它们与“温度”成比例；\n",
    "每次迭代温度都会降低，扰动边界会减少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:51:22.120431Z",
     "start_time": "2017-09-30T19:51:22.094413Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0,len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, int(round(temperature)))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:53:12.993592Z",
     "start_time": "2017-09-30T19:53:09.506189Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "59 ['doy', 'ouseeth', 'ekitt', 'y', 'se', 'eth', 'edoggy', 'doyou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "59 ['doy', 'ouseeth', 'ekitt', 'y', 'se', 'eth', 'edoggy', 'doyou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "59 ['doy', 'ouseeth', 'ekitt', 'y', 'se', 'eth', 'edoggy', 'doyou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "59 ['doy', 'ouseeth', 'ekitt', 'y', 'se', 'eth', 'edoggy', 'doyou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "59 ['doy', 'ouseeth', 'ekitt', 'y', 'se', 'eth', 'edoggy', 'doyou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "59 ['doy', 'ouseeth', 'ekitt', 'y', 'se', 'eth', 'edoggy', 'doyou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "57 ['doy', 'ouseeth', 'ekitt', 'y', 'se', 'eth', 'edoggy', 'doy', 'ou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "51 ['doy', 'ou', 'seeth', 'ekitt', 'y', 'seeth', 'edoggy', 'doy', 'ou', 'liket', 'h', 'ekitt', 'y', 'liket', 'h', 'edoggy']\n",
      "51 ['doy', 'ou', 'seeth', 'ekitt', 'y', 'seeth', 'edoggy', 'doy', 'ou', 'liket', 'h', 'ekitt', 'y', 'liket', 'h', 'edoggy']\n",
      "51 ['doy', 'ou', 'seeth', 'ekitt', 'y', 'seeth', 'edoggy', 'doy', 'ou', 'liket', 'h', 'ekitt', 'y', 'liket', 'h', 'edoggy']\n",
      "48 ['doyou', 'seeth', 'ekitt', 'y', 'seeth', 'edoggy', 'doyou', 'liket', 'h', 'ekitt', 'y', 'liket', 'h', 'edoggy']\n",
      "45 ['doyou', 'seeth', 'ekitt', 'y', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitt', 'y', 'liketh', 'edoggy']\n",
      "42 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "42 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "42 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "42 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "42 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "42 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "42 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n"
     ]
    }
   ],
   "source": [
    "res = anneal(text, seg1, iterations=2000, cooling_rate=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T19:53:16.134812Z",
     "start_time": "2017-09-30T19:53:16.113798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后的词:\n",
      " ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "\n",
      "分词后词的个数:  10\n",
      "分词后加上空格的句子总长度:  32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, res, show_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了足够的数据，就可能以一个合理的准确度自动将文本分割成词汇。这种方法可用于\n",
    "为那些词的边界没有任何视觉表示的书写系统分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附录一：去除文本前后特殊性文本的trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-29T23:25:47.544788Z",
     "start_time": "2017-09-29T23:25:47.533261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states \\n– called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov \\ninformation sources and hidden Markov models.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比如只我想得到 PART1到 END OF PARER之间的文本\n",
    "test = '''\n",
    "*x**x*x**x**x* PART1\n",
    "The Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states \n",
    "– called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov \n",
    "information sources and hidden Markov models.\n",
    "END OF PAPER....\n",
    "'''\n",
    "beg = test.find('PART1')+len('PART1')\n",
    "end = test.rfind('END OF PAPE') #rfind是反向地遍历，对于大文本来说是个省时间的操作\n",
    "\n",
    "test[beg:end].strip()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
